{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KenLM Paragraph-base\n",
    "\n",
    "\n",
    "#### based on:\n",
    "\n",
    "https://kheafield.com/papers/avenue/kenlm.pdf \n",
    "\n",
    "https://kheafield.com/papers/edinburgh/estimate_paper.pdf\n",
    "\n",
    "#### implementation of the estimation part:\n",
    "\n",
    "https://kheafield.com/code/kenlm/\n",
    "\n",
    "Language models are estimated from text using __[modified](http://www.ee.columbia.edu/~stanchen/papers/h015a-techreport.pdf)__ [Kneser-Ney smoothing](https://ieeexplore.ieee.org/document/479394) without pruning. It is done on disk, enabling one to build much larger models. Kneser-Ney smothng consistently outperforms all other n-grams models with smothing evaluated in this [techreport](http://www.ee.columbia.edu/~stanchen/papers/h015a-techreport.pdf) by Chen and Goodman.\n",
    "\n",
    "In this notebook I am splitting and tokenizing __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) into texts including symbols. Then I use KenLM scripts to estimate ARPA n-gram models. KenLM script __lmplz__ by default uses $<s>$ and $</s>$ tags at the beginning and end of each sentence. In my case it sees the whole text as a sentence.\n",
    "\n",
    "It is the text-based model and it can estimates score for the text. The score is equal to the  log10 probability of the text. Then I sum up scores for all the texts in the corpus, divide them by the number of the words in the corpus and take 10 to the power of the resulting fraction to calculate the __perplexity__ of my model.\n",
    "\n",
    "## Installing KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ---# sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev \n",
    "    \n",
    "    ---# wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "         mkdir -p build && cd build\n",
    "         cmake ..\n",
    "         make -j 4\n",
    "         \n",
    "## Tokenize text\n",
    "         \n",
    "Here the __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) is splitted into texts including symbols. Also $<SENTENCE></SENTENCE>$ tags are added at the beginning and at the and of each sentence. Also paragraphs are splitted by $PARAGRAPH$ tag. Then $BEGIN\\_TEXT$ and $END\\_TEXT$ tags wrap all the texts. Then everything is lowercased.\n",
    "         \n",
    "_!!! Should I lowercase? !!!_         \n",
    "\n",
    "#### [TokenizeText.groovy](https://github.com/brown-uk/nlp_uk/blob/master/src/main/groovy/org/nlp_uk/tools/README.md)\n",
    "\n",
    "\n",
    "Аналізує текст і записує результат у виходовий файл:\n",
    "\n",
    "        розбиває на речення (-s)\n",
    "        розбиває на токени (-w) (результати включають пунктуацію тому всі токени розділяються вертикальними рисками)\n",
    "        розбиває на слова (-u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.codehaus.groovy.vmplugin.v7.Java7$1 (file:/snap/groovy/17/lib/groovy-2.5.8.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.vmplugin.v7.Java7$1\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "writing into brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt\n"
     ]
    }
   ],
   "source": [
    "!groovy nlp_uk/nlp_uk/src/main/groovy/org/nlp_uk/tools/TokenizeText.groovy -s -w -i brown-uk/corpus/all_GS_septexts.txt -o brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "filename=os.path.join(os.path.abspath(''), \n",
    "                      'brown-uk','corpus',\n",
    "                      'all_GS_words_symbols_paragraph_text.txt')\n",
    "for line in fileinput.FileInput(filename, inplace=1):\n",
    "        line='<SENTENCE> '+line\n",
    "        line=line.replace(\"|\",\" \")\n",
    "        #line=line.replace(\"\\n\",\"\")\n",
    "        #line=line.replace(r\"\\n\",\"\")\n",
    "        line=line.replace(\"\\n\",\" </SENTENCE> \")\n",
    "        line=line.replace(r\"<SENTENCE>\\n\",\"PARAGRAPH <SENTENCE> \")\n",
    "        line=line.replace(\"<SENTENCE></SENTENCE>\",\" \")\n",
    "        line=line.replace(r\"\\n </SENTENCE>\",\" </SENTENCE> PARAGRAPH\")\n",
    "        line=line.replace(r\"\\n\",\" </SENTENCE> <SENTENCE> \")\n",
    "        line=line.replace(r\"•\",\"\")\n",
    "        line=line.replace(\"<SENTENCE>  BEGIN_TEXT\",\"BEGIN_TEXT <SENTENCE>\")\n",
    "        line=line.replace(\"<SENTENCE> BEGIN_TEXT\",\"BEGIN_TEXT <SENTENCE>\")\n",
    "        line=line.replace(\"END_TEXT  </SENTENCE>\",\"</SENTENCE>  END_TEXT\")\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = open(filename,'r').read().split('\\n')\n",
    "mystr=' '.join(temp).replace(\"END_TEXT BEGIN_TEXT\",\"END_TEXT \\nBEGIN_TEXT\").replace(\"<SENTENCE>   </SENTENCE>\",\" \").replace(\"<SENTENCE>  </SENTENCE>\",\" \").replace(\"<SENTENCE> </SENTENCE>\",\" \").lower()\n",
    "with open(filename, 'w') as out_file:\n",
    "     out_file.write(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Example result file:\n",
    "\n",
    "     begin_text ... <sentence> у   2013   році ,   до   100-річчя   виходу   першого   числа   журналу   _foreign_ ,   на   будинку   встановили   меморіальну   дошку .    </sentence>  <sentence> тоді   ж   таки   в   будинку   відбувся   перший   з'їзд   есперантистів ,   на   якому   було   50   есперантистів   з   усієї   україни   і   троє   з-за   кордону .    </sentence>  <sentence> відтоді   щороку   вони   там   організовують   конференції ,   починаючи   з   2013-го .    </sentence>  <sentence> щороку   там   вручають   премію   тим ,   хто   пропаґує   український   есперантський   рух   та   український   погляд   на   важливі   події .  </sentence>    <sentence> mirinda   lviv  </sentence> <sentence>  більшість   дописів   на   сторінці   сергія   шматкова   у   соціальній   мережі   _foreign_   —   мовою   есперанто .  </sentence>    <sentence> « у   мене   у   _foreign_   понад   дві   тисячі   друзів   з   усього   світу ,   з   якими   я   спілкуюсь   мовою   есперанто » ,   —   розповідає   пан   сергій .  </sentence>    <sentence> цю   незвичну   для   багатьох   мову   сергій   шматков   вивчив   ще   у   1980-х .    </sentence>  <sentence> народився   і   прожив   чоловік   усе   життя   в   донецькій   області ,   а   після   окупації   перебрався   до   львова .    </sentence>  ... paragraph ... paragraph ...end_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Large Language Models with KenLM\n",
    "\n",
    "Tokenized and splitted into texts __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) is provided on stdin and the __ARPA__ is written to stdout.\n",
    "\n",
    "#### kenlm/build/bin/lmplz -o -S -T    \n",
    "        -o\n",
    "            Required. Order of the language model to estimate.\n",
    "        -S\n",
    "            Recommended. Memory to use. This is a number followed by single-character suffix: % for percentage of physical memory (on platforms where this is measured), b for bytes, K for kilobytes, M for megabytes, and so on for G and T. If no suffix is given, kilobytes are assumed for compatability with GNU sort. The sort program is not used; the command line is simply designed to be compatible.\n",
    "        -T\n",
    "            Recommended. Temporary file location.\n",
    "\n",
    "Here 3-gram, 4-gram, 5-gram and 6-gram models are estimated by kenlm library and saved into the propriate ARPA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/ana/Downloads/master diploma/code/brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 817744 types 96700\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1160400 2:580715456 3:1088841600\n",
      "Statistics:\n",
      "1 96700 D1=0.648565 D2=1.09892 D3+=1.53524\n",
      "2 446271 D1=0.852958 D2=1.22651 D3+=1.49885\n",
      "3 646761 D1=0.919303 D2=1.33588 D3+=1.37954\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 24283 assuming -p 1.5\n",
      "probing 27276 assuming -r models -p 1.5\n",
      "trie    11503 without quantization\n",
      "trie     7130 assuming -q 8 -b 8 quantization \n",
      "trie    10820 assuming -a 22 array pointer compression\n",
      "trie     6446 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:1823112 kB\tVmRSS:28828 kB\tRSSMax:412668 kB\tuser:1.17891\tsys:0.342521\tCPU:1.5215\treal:1.29736\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/ana/Downloads/master diploma/code/brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 817744 types 96700\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1160400 2:284179904 3:532837312 4:852539712\n",
      "Statistics:\n",
      "1 96700 D1=0.648565 D2=1.09892 D3+=1.53524\n",
      "2 446271 D1=0.852958 D2=1.22651 D3+=1.49885\n",
      "3 646761 D1=0.944978 D2=1.32105 D3+=1.49591\n",
      "4 730263 D1=0.951508 D2=1.39163 D3+=1.2968\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 40909 assuming -p 1.5\n",
      "probing 47692 assuming -r models -p 1.5\n",
      "trie    19888 without quantization\n",
      "trie    11571 assuming -q 8 -b 8 quantization \n",
      "trie    18188 assuming -a 22 array pointer compression\n",
      "trie     9871 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:1813924 kB\tVmRSS:9476 kB\tRSSMax:368224 kB\tuser:1.77746\tsys:0.516935\tCPU:2.29445\treal:1.78441\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/ana/Downloads/master diploma/code/brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 817744 types 96700\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1160400 2:162883600 3:305406784 4:488650816 5:712615808\n",
      "Statistics:\n",
      "1 96700 D1=0.648565 D2=1.09892 D3+=1.53524\n",
      "2 446271 D1=0.852958 D2=1.22651 D3+=1.49885\n",
      "3 646761 D1=0.944978 D2=1.32105 D3+=1.49591\n",
      "4 730263 D1=0.971448 D2=1.29829 D3+=1.24324\n",
      "5 785884 D1=0.969985 D2=1.56779 D3+=1.42172\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 57 assuming -p 1.5\n",
      "probing 68 assuming -r models -p 1.5\n",
      "trie    28 without quantization\n",
      "trie    16 assuming -q 8 -b 8 quantization \n",
      "trie    25 assuming -a 22 array pointer compression\n",
      "trie    13 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312 5:22004752\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312 5:22004752\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:1830320 kB\tVmRSS:9872 kB\tRSSMax:334800 kB\tuser:2.57018\tsys:0.679291\tCPU:3.24952\treal:2.35618\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/ana/Downloads/master diploma/code/brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 817744 types 96700\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1160400 2:102741968 3:192641200 4:308225920 5:449496128 6:616451840\n",
      "Statistics:\n",
      "1 96700 D1=0.648565 D2=1.09892 D3+=1.53524\n",
      "2 446271 D1=0.852958 D2=1.22651 D3+=1.49885\n",
      "3 646761 D1=0.944978 D2=1.32105 D3+=1.49591\n",
      "4 730263 D1=0.971448 D2=1.29829 D3+=1.24324\n",
      "5 785884 D1=0.985398 D2=1.42333 D3+=1.36936\n",
      "6 803302 D1=0.981966 D2=1.75057 D3+=1.50639\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 75 assuming -p 1.5\n",
      "probing 91 assuming -r models -p 1.5\n",
      "trie    37 without quantization\n",
      "trie    21 assuming -q 8 -b 8 quantization \n",
      "trie    33 assuming -a 22 array pointer compression\n",
      "trie    17 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312 5:22004752 6:25705664\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1160400 2:7140336 3:12935220 4:17526312 5:22004752 6:25705664\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:1846712 kB\tVmRSS:9604 kB\tRSSMax:308628 kB\tuser:3.46443\tsys:0.826966\tCPU:4.29143\treal:2.9074\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 3 -S 10% <brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt> brown-uk/corpus/kenlm/all_GS_symbols_paragraph_text_based_3.arpa\n",
    "!kenlm/build/bin/lmplz -o 4 -S 10% <brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt> brown-uk/corpus/kenlm/all_GS_symbols_paragraph_text_based_4.arpa\n",
    "!kenlm/build/bin/lmplz -o 5 -S 10% <brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt> brown-uk/corpus/kenlm/all_GS_symbols_paragraph_text_based_5.arpa\n",
    "!kenlm/build/bin/lmplz -o 6 -S 10% <brown-uk/corpus/all_GS_words_symbols_paragraph_text.txt> brown-uk/corpus/kenlm/all_GS_symbols_paragraph_text_based_6.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM3 = os.path.join(os.path.abspath(''), 'brown-uk', 'corpus', 'kenlm', 'all_GS_symbols_paragraph_text_based_3.arpa')\n",
    "LM4 = os.path.join(os.path.abspath(''), 'brown-uk', 'corpus', 'kenlm', 'all_GS_symbols_paragraph_text_based_4.arpa')\n",
    "LM5 = os.path.join(os.path.abspath(''), 'brown-uk', 'corpus', 'kenlm', 'all_GS_symbols_paragraph_text_based_5.arpa')\n",
    "LM6 = os.path.join(os.path.abspath(''), 'brown-uk', 'corpus', 'kenlm', 'all_GS_symbols_paragraph_text_based_6.arpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only up to 6 :( \n",
    "\n",
    "If more then 6 then ERROR:\n",
    "\n",
    "#### RuntimeError                              Traceback (most recent call last)\n",
    "    kenlm.pyx in kenlm.Model.__init__()\n",
    "\n",
    "    RuntimeError: lm/model.cc:49 in void lm::ngram::detail::{anonymous}::CheckCounts(const std::vector<long unsigned int>&) threw FormatLoadException because `counts.size() > 6'.\n",
    "    This model has order 7 but KenLM was compiled to support up to 6.  If your build system supports changing KENLM_MAX_ORDER, change it there and recompile.  With cmake:\n",
    "     cmake -DKENLM_MAX_ORDER=10 ..\n",
    "    With Moses:\n",
    "     bjam --max-kenlm-order=10 -a\n",
    "    Otherwise, edit lm/max_order.hh. Byte: 113\n",
    "#### OSError: Cannot read model '/home/ana/Downloads/master diploma/code/brown-uk/corpus/all_GS_symbols_paragraph_text_based_7.arpa' \n",
    "    (lm/model.cc:49 in void lm::ngram::detail::{anonymous}::CheckCounts(const std::vector<long unsigned int>&) threw FormatLoadException because `counts.size() > 6'. This model has order 7 but KenLM was compiled to support up to 6.  If your build system supports changing KENLM_MAX_ORDER, change it there and recompile.  With cmake:  cmake -DKENLM_MAX_ORDER=10 .. With Moses:  bjam --max-kenlm-order=10 -a Otherwise, edit lm/max_order.hh. Byte: 113)\n",
    "    \n",
    "    \n",
    "###### _!!! If needed can try later to fix and estimate 9-gram!!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = kenlm.LanguageModel(LM3)\n",
    "model4 = kenlm.LanguageModel(LM4)\n",
    "model5 = kenlm.LanguageModel(LM5)\n",
    "model6 = kenlm.LanguageModel(LM6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences scores\n",
    "\n",
    "#### model.score(self, sentence, bos = True, eos = True)\n",
    "\n",
    "Return the __log10 probability of a string__.  By default, the string is treated as a sentence.  \n",
    "          \n",
    "          return log10 p(sentence </s> | <s>)\n",
    "\n",
    "If you do not want to condition on the beginning of sentence, pass __bos = False__ . Never include $<s>$ as part of the string. \n",
    "\n",
    "Similarly, the end of sentence token $</s>$ can be omitted with __eos = False__. Since language models explicitly predict $</s>$, it can be part of the string.\n",
    "\n",
    "\n",
    "I do use bos or eos = False, so the method calculates scores of those strings. I artificially add  $<SENTENCE> </SENTENCE> $ tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sentence> штучний інтелект врятує світ . </sentence> \n",
      "3-gram model\n",
      "-23.78922462463379\n",
      "4-gram model\n",
      "-23.777435302734375\n",
      "5-gram model\n",
      "-23.777435302734375\n",
      "6-gram model\n",
      "-23.777435302734375\n"
     ]
    }
   ],
   "source": [
    "sentence1 = '<SENTENCE> Штучний інтелект врятує світ . </SENTENCE> '\n",
    "sentence1 = sentence1.lower()\n",
    "print(sentence1)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence1, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(model4.score(sentence1, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(model5.score(sentence1, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence1, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sentence> _foreign_ врятує світ . </sentence>\n",
      "3-gram model\n",
      "-17.128862380981445\n",
      "4-gram model\n",
      "-16.742843627929688\n",
      "5-gram model\n",
      "-16.742843627929688\n",
      "6-gram model\n",
      "-16.742843627929688\n"
     ]
    }
   ],
   "source": [
    "sentence2 = '<SENTENCE> _FOREIGN_ врятує світ . </SENTENCE>'\n",
    "sentence2 = sentence2.lower()\n",
    "print(sentence2)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence2, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(model4.score(sentence2, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(model5.score(sentence2, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence2, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sentence> наука врятує світ . </sentence>\n",
      "3-gram model\n",
      "-17.484424591064453\n",
      "4-gram model\n",
      "-17.46067237854004\n",
      "5-gram model\n",
      "-17.46067237854004\n",
      "6-gram model\n",
      "-17.46067237854004\n"
     ]
    }
   ],
   "source": [
    "sentence3 = '<SENTENCE> Наука врятує світ . </SENTENCE>'\n",
    "sentence3 = sentence3.lower()\n",
    "print(sentence3)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence3, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(model4.score(sentence3, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(model5.score(sentence3, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence3, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_text <sentence> краса врятує світ . </sentence> end_text\n",
      "3-gram model\n",
      "-16.80482292175293\n",
      "4-gram model\n",
      "-17.075908660888672\n",
      "5-gram model\n",
      "-16.987186431884766\n",
      "6-gram model\n",
      "-16.987186431884766\n"
     ]
    }
   ],
   "source": [
    "sentence4 = 'BEGIN_TEXT <SENTENCE> Краса врятує світ . </SENTENCE> END_TEXT'\n",
    "sentence4 = sentence4.lower()\n",
    "print(sentence4)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(model4.score(sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(model5.score(sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence4, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that total full score = direct score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, s):\n",
    "    return sum(prob for prob, _, _ in model.full_scores(s, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (abs(score(model6, sentence1) - model6.score(sentence1, bos=False, eos=False)) < 1e-3)\n",
    "assert (abs(score(model6, sentence2) - model6.score(sentence2, bos=False, eos=False)) < 1e-3)\n",
    "assert (abs(score(model6, sentence3) - model6.score(sentence3, bos=False, eos=False)) < 1e-3)\n",
    "assert (abs(score(model6, sentence4) - model6.score(sentence4, bos=False, eos=False)) < 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show scores and n-gram matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.535782814025879 1: <sentence>\n",
      "-0.8325477242469788 2: <sentence> краса\n",
      "-4.5061163902282715 2: краса врятує\n",
      "-1.9362119436264038 2: врятує світ\n",
      "-0.9934155344963074 3: врятує світ .\n",
      "-0.9698286652565002 2: . </sentence>\n",
      "-0.0004917234182357788 3: . </sentence> end_text\n",
      "-2.2127907276153564 4: . </sentence> end_text\n"
     ]
    }
   ],
   "source": [
    "# Show scores and n-gram matches\n",
    "words = sentence4.split()\n",
    "for i, (prob, length, oov) in enumerate(model6.full_scores(sentence4, bos=False, eos=False)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gramm які закінчуютьмя на $</s>$ завжди друга цифра нуль. Виходить нема залежності між реченнями. Як на мене це не ок. Саме тому зробила всю штуку з реченнями, параграфами, текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the perplexity of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, sentence, bos=False, eos=False):\n",
    "    \"\"\"\n",
    "    Compute perplexity of a sentence.\n",
    "    @param sentence One full sentence to score.  Do not include <s> or </s>.\n",
    "    \"\"\"\n",
    "    words = len(str(sentence).split())# For </s>\n",
    "    return 10.0**(-model.score(sentence, bos, eos) / words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(sentence4).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n",
      "126.06742006826272\n",
      "4-gram model\n",
      "136.29771737354372\n",
      "5-gram model\n",
      "132.86124078652648\n",
      "6-gram model\n",
      "132.86124078652648\n"
     ]
    }
   ],
   "source": [
    "print('{0}-gram model'.format(model3.order))\n",
    "print(perplexity(model3, sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(perplexity(model4, sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(perplexity(model5, sentence4, bos=False, eos=False))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(perplexity(model6, sentence4, bos=False, eos=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if not w in model6:\n",
    "        print('\"{0}\" is an OOV'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perplexity of the model on the Ukrainian brown corpus (good and so-so)\n",
    "\n",
    "_!!! keep in mind this is the same dataset I made my estimation ARPA models on !!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=os.path.join(os.path.abspath(''), \n",
    "                      'brown-uk', 'corpus',\n",
    "                      'all_GS_words_symbols_paragraph_text.txt')\n",
    "temp = open(filename,'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the text-based model and it can estimates score for the text. The score is equal to the  log10 probability of the text. Thus, I iterate through the texts and calculate the scores. Then I sum up scores for all the texts in the corpus, divide them by the number of the words in the corpus and take 10 to the power of the resulting fraction to calculate the __perplexity__ of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_on_texts(model, temp, bos=False, eos=False):\n",
    "    all_score=0\n",
    "    all_words=0\n",
    "    for text in temp:\n",
    "        all_score+=model.score(text, bos=bos, eos=eos)\n",
    "        all_words+=len(str(text).split())\n",
    "    print(\"all_score: \"+str(all_score)+\"; all_words: \"+str(all_words))\n",
    "    return 10.0**(-all_score / all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n",
      "perplexity: 16.564665016290835\n",
      "\n",
      "4-gram model\n",
      "perplexity: 10.34141724464607\n",
      "\n",
      "5-gram model\n",
      "perplexity: 8.004362326244326\n",
      "\n",
      "6-gram model\n",
      "perplexity: 7.218153155757558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{0}-gram model'.format(model3.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model3,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model4,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(\"perplexity: \"+ str(perplexity_on_texts(model5,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model6,temp, bos=False, eos=False))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n",
      "all_score: -996924.4399871826; all_words: 817699\n",
      "perplexity: 16.564665016290835\n",
      "\n",
      "4-gram model\n",
      "all_score: -829621.1012496948; all_words: 817699\n",
      "perplexity: 10.34141724464607\n",
      "\n",
      "5-gram model\n",
      "all_score: -738649.3713378906; all_words: 817699\n",
      "perplexity: 8.004362326244326\n",
      "\n",
      "6-gram model\n",
      "all_score: -701934.1576080322; all_words: 817699\n",
      "perplexity: 7.218153155757558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{0}-gram model'.format(model3.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model3,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model4.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model4,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model5.order))\n",
    "print(\"perplexity: \"+ str(perplexity_on_texts(model5,temp, bos=False, eos=False))+\"\\n\")\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(\"perplexity: \"+str(perplexity_on_texts(model6,temp, bos=False, eos=False))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
