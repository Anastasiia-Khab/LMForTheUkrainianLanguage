{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KenLM Sentence-base\n",
    "\n",
    "#### based on:\n",
    "\n",
    "https://kheafield.com/papers/avenue/kenlm.pdf \n",
    "\n",
    "https://kheafield.com/papers/edinburgh/estimate_paper.pdf\n",
    "\n",
    "#### implementation of the estimation part:\n",
    "\n",
    "https://kheafield.com/code/kenlm/\n",
    "\n",
    "Language models are estimated from text using __[modified](http://www.ee.columbia.edu/~stanchen/papers/h015a-techreport.pdf)__ [Kneser-Ney smoothing](https://ieeexplore.ieee.org/document/479394) without pruning. It is done on disk, enabling one to build much larger models. Kneser-Ney smothng consistently outperforms all other n-grams models with smothing evaluated in this [techreport](http://www.ee.columbia.edu/~stanchen/papers/h015a-techreport.pdf) by Chen and Goodman.\n",
    "\n",
    "In this notebook I am splitting and tokenizing __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) into sentences including symbols. Then I use KenLM scripts to estimate ARPA n-gram sentense-based models. KenLM script __lmplz__ by default uses $<s>$ and $</s>$ tags at the beginning and end of each sentence. \n",
    "\n",
    "It is only sentence-based model and it estimates score for each sentence separately. The score is equal to the  log10 probability of the sentence. Then I sum up scores for all the sentences in the corpus, divide them by the number of the words in the corpus and take 10 to the power of the resulting fraction to calculate the __perplexity__ of my model.\n",
    "\n",
    "## Installing KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kenlm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown option: --\r\n",
      "usage: python3 [option] ... [-c cmd | -m mod | file | -] [arg] ...\r\n",
      "Try `python -h' for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --vesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"kenlm\", \"/home/nastuha97/.local/lib/python3.6/site-packages/kenlm.cpython-36m-x86_64-linux-gnu.so\")\n",
    "kenlm = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(kenlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nastuha97/.local/lib/python3.6/site-packages/kenlm.cpython-36m-x86_64-linux-gnu.so'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kenlm.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ---# sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev \n",
    "    \n",
    "    ---# wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "         mkdir -p build && cd build\n",
    "         cmake ..\n",
    "         make -j 4\n",
    "         \n",
    "## Tokenize text\n",
    "         \n",
    "Here the __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) is splitted into sentences including symbols.\n",
    "         \n",
    "#### [TokenizeText.groovy](https://github.com/brown-uk/nlp_uk/blob/master/src/main/groovy/org/nlp_uk/tools/README.md)\n",
    "\n",
    "\n",
    "Аналізує текст і записує результат у виходовий файл:\n",
    "\n",
    "        розбиває на речення (-s)\n",
    "        розбиває на токени (-w) (результати включають пунктуацію тому всі токени розділяються вертикальними рисками)\n",
    "        розбиває на слова (-u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.codehaus.groovy.vmplugin.v7.Java7$1 (file:/snap/groovy/17/lib/groovy-2.5.8.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.vmplugin.v7.Java7$1\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "writing into final/korr_final_symbols_sentences.txt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!groovy nlp_uk/nlp_uk/src/main/groovy/org/nlp_uk/tools/TokenizeText.groovy -s -w -i final/korr_final.txt -o final/korr_final_symbols_sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_!!! Should I lowercase? !!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "filename=os.path.join(os.path.abspath(''), 'final','korr_final_symbols_sentences.txt')\n",
    "for line in fileinput.FileInput(filename, inplace=1):\n",
    "        #line='<s>'+line\n",
    "        line=line.replace(\"|\",\" \")#.lower()\n",
    "        line=line.replace(\"\\n\",\"\")\n",
    "        line=line.replace(r\"\\n\",\"\")\n",
    "        line=line.replace(\"_foreign_\",\" _foreign_ \")\n",
    "        #line=line.replace(\"BEGIN_TEXT\",\"\")\n",
    "        #line=line.replace(\"END_TEXT\",\"\")\n",
    "        #line=line.replace(\"\\n\",\"</s>\")\n",
    "        #line=line.replace(r\"<s>\\n\",\"BEGIN <s>\")\n",
    "        #line=line.replace(\"<s></s>\",\"\")\n",
    "        #line=line.replace(r\"\\n </s>\",\"</s> END\")\n",
    "        #line=line.replace(\"<s>\",\"\")\n",
    "        print (line)\n",
    "#for line in fileinput.FileInput(filename, inplace=1):\n",
    "        #line=line.replace(\"\\n\",\"\")\n",
    "        #print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example result file: (+lowercase)\n",
    "\n",
    "    У   2013   році ,   до   100-річчя   виходу   першого   числа   журналу   _FOREIGN_ ,   на   будинку   встановили   меморіальну   дошку .   \n",
    "    Тоді   ж   таки   в   будинку   відбувся   перший   з'їзд   есперантистів ,   на   якому   було   50   есперантистів   з   усієї   України   і   троє   з-за   кордону .   \n",
    "    Відтоді   щороку   вони   там   організовують   конференції ,   починаючи   з   2013-го .   \n",
    "    Щороку   там   вручають   премію   тим ,   хто   пропаґує   український   есперантський   рух   та   український   погляд   на   важливі   події .  \n",
    "    _FOREIGN_  Більшість   дописів   на   сторінці   Сергія   Шматкова   у   соціальній   мережі   _FOREIGN_   —   мовою   есперанто .  \n",
    "    « У   мене   у   _FOREIGN_   понад   дві   тисячі   друзів   з   усього   світу ,   з   якими   я   спілкуюсь   мовою   есперанто » ,   —   розповідає   пан   Сергій .  \n",
    "    Цю   незвичну   для   багатьох   мову   Сергій   Шматков   вивчив   ще   у   1980-х .   \n",
    "    Народився   і   прожив   чоловік   усе   життя   в   Донецькій   області ,   а   після   окупації   перебрався   до   Львова .  \n",
    "    \n",
    "    \n",
    "#### Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final/korr_final_symbols_sentences.txt\", \"r\") as input:\n",
    "    summa=0\n",
    "    for line in input:\n",
    "        summa+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8640598"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Large Language Models with KenLM\n",
    "\n",
    "Tokenized and splitted into sentences __[Ukrainian Brown Corpus](https://github.com/brown-uk/corpu)__ (good and so-so datasets) is provided on stdin and the __ARPA__ is written to stdout.\n",
    "\n",
    "#### kenlm/build/bin/lmplz -o -S -T    \n",
    "        -o\n",
    "            Required. Order of the language model to estimate.\n",
    "        -S\n",
    "            Recommended. Memory to use. This is a number followed by single-character suffix: % for percentage of physical memory (on platforms where this is measured), b for bytes, K for kilobytes, M for megabytes, and so on for G and T. If no suffix is given, kilobytes are assumed for compatability with GNU sort. The sort program is not used; the command line is simply designed to be compatible.\n",
    "        -T\n",
    "            Recommended. Temporary file location.\n",
    "\n",
    "Here 3-gram, 4-gram, 5-gram and 6-gram models are estimated by kenlm library and saved into the propriate ARPA files.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----!kenlm/build/bin/lmplz -o 4 -S 10% <final/korr_final_symbols_sentences.txt> final/kenlm/korr_final_symbols_sentences_based_4.arpa\n",
    "#----!kenlm/build/bin/lmplz -o 5 -S 10% <final/korr_final_symbols_sentences.txt> final/kenlm/korr_final_symbols_sentences_based_5.arpa\n",
    "print(datetime.datetime.now())\n",
    "!kenlm/build/bin/lmplz -o 6 -S 10% <final/uk_final_symbols_sentences.txt> final/kenlm/uk_final_symbols_sentences_based_6.arpa \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ````kenlm/build/bin/lmplz -o 6 -S 10% <final/uk_final_symbols_sentences.txt> final/kenlm\n",
    "    /uk_final_symbols_sentences_based_6.arpa \n",
    "    === 1/5 Counting and sorting n-grams ===\n",
    "    Reading /home/nastuha97/master/final/uk_final_symbols_sentences.txt\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ****************************************************************************************************\n",
    "    Unigram tokens 241313710 types 2261926\n",
    "    === 2/5 Calculating and sorting adjusted counts ===\n",
    "    Chain sizes: 1:27143112 2:665933312 3:1248624896 4:1997799936 5:2913458176 6:3995599872\n",
    "    Statistics:\n",
    "    1 2261926 D1=0.67269 D2=1.02208 D3+=1.35105\n",
    "    2 36830739 D1=0.771578 D2=1.10498 D3+=1.36663\n",
    "    3 101599818 D1=0.857522 D2=1.18796 D3+=1.36978\n",
    "    4 148498582 D1=0.913768 D2=1.28426 D3+=1.39683\n",
    "    5 167508186 D1=0.947499 D2=1.39375 D3+=1.45894\n",
    "    6 169630105 D1=0.889727 D2=1.52732 D3+=1.59437\n",
    "    Memory estimate for binary LM:\n",
    "    type       MB\n",
    "    probing 13369 assuming -p 1.5\n",
    "    probing 15978 assuming -r models -p 1.5\n",
    "    trie     7240 without quantization\n",
    "    trie     4229 assuming -q 8 -b 8 quantization \n",
    "    trie     6121 assuming -a 22 array pointer compression\n",
    "    trie     3110 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
    "    === 3/5 Calculating and sorting initial probabilities ===\n",
    "    Chain sizes: 1:27143112 2:589291824 3:1274550272 4:2039280512 5:2973950464 6:4078561024\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ####################################################################################################\n",
    "    === 4/5 Calculating and writing order-interpolated probabilities ===\n",
    "    Chain sizes: 1:27143112 2:589291824 3:1150784000 4:1841254272 5:2685162240 6:3682508544\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ####################################################################################################\n",
    "    === 5/5 Writing ARPA model ===\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ****************************************************************************************************\n",
    "    Name:lmplz      VmPeak:11891512 kB      VmRSS:65996 kB  RSSMax:10731432 kB      user:853.716    sys:177.131     CPU\n",
    "    :1030.85        real:1955.51```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 17:34:29.146266\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/nastuha97/master/final/uk_final_symbols_sentences.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 241313710 types 2261926\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:27143112 2:3810655232 3:7144978944\n",
      "Statistics:\n",
      "1 2261926 D1=0.67269 D2=1.02208 D3+=1.35105\n",
      "2 36830739 D1=0.771578 D2=1.10498 D3+=1.36663\n",
      "3 101599818 D1=0.786184 D2=1.25763 D3+=1.43014\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 2643 assuming -p 1.5\n",
      "probing 2862 assuming -r models -p 1.5\n",
      "trie    1185 without quantization\n",
      "trie     700 assuming -q 8 -b 8 quantization \n",
      "trie    1103 assuming -a 22 array pointer compression\n",
      "trie     618 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:27143112 2:589291824 3:2031996360\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:27143112 2:589291824 3:2031996360\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:10916916 kB\tVmRSS:65512 kB\tRSSMax:4513564 kB\tuser:197.61\tsys:25.0719\tCPU:222.682\treal:204.184\n",
      "2020-01-01 17:37:53.474297\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "!kenlm/build/bin/lmplz -o 3 -S 10% <final/uk_final_symbols_sentences.txt> final/kenlm/uk_final_symbols_sentences_based_3.arpa \n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2019-12-07 11:56:19.000665\n",
    "    === 1/5 Counting and sorting n-grams ===\n",
    "    Reading /home/nastuha97/master/final/uk_final_symbols_sentences.txt\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ****************************************************************************************************\n",
    "    Unigram tokens 241313710 types 2261926\n",
    "    === 2/5 Calculating and sorting adjusted counts ===\n",
    "    Chain sizes: 1:27143112 2:3810655232 3:7144978944\n",
    "    Statistics:\n",
    "    1 2261926 D1=0.67269 D2=1.02208 D3+=1.35105\n",
    "    2 36830739 D1=0.771578 D2=1.10498 D3+=1.36663\n",
    "    3 101599818 D1=0.786184 D2=1.25763 D3+=1.43014\n",
    "    Memory estimate for binary LM:\n",
    "    type      MB\n",
    "    probing 2643 assuming -p 1.5\n",
    "    probing 2862 assuming -r models -p 1.5\n",
    "    trie    1185 without quantization\n",
    "    trie     700 assuming -q 8 -b 8 quantization \n",
    "    trie    1103 assuming -a 22 array pointer compression\n",
    "    trie     618 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
    "    === 3/5 Calculating and sorting initial probabilities ===\n",
    "    Chain sizes: 1:27143112 2:589291824 3:2031996360\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ####################################################################################################\n",
    "    === 4/5 Calculating and writing order-interpolated probabilities ===\n",
    "    Chain sizes: 1:27143112 2:589291824 3:2031996360\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ####################################################################################################\n",
    "    === 5/5 Writing ARPA model ===\n",
    "    ----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
    "    ****************************************************************************************************\n",
    "    Name:lmplz\tVmPeak:10916916 kB\tVmRSS:65880 kB\tRSSMax:4514276 kB\tuser:229.414\tsys:25.4962\tCPU:254.91\treal:234.975\n",
    "    2019-12-07 12:00:14.091664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM3 = os.path.join(os.path.abspath(''), 'final', 'kenlm', 'uk_final_symbols_sentences_based_3.arpa')\n",
    "#LM4 = os.path.join(os.path.abspath(''), 'final', 'kenlm', 'korr_final_symbols_sentences_based_4.arpa')\n",
    "#LM5 = os.path.join(os.path.abspath(''), 'final', 'kenlm', 'korr_final_symbols_sentences_based_5.arpa')\n",
    "LM6 = os.path.join(os.path.abspath(''), 'final', 'kenlm', 'uk_final_symbols_sentences_based_6.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip (539kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): kenlm==0.0.0 from https://github.com/kpu/kenlm/archive/master.zip in /home/nastuha97/.local/lib/python3.6/site-packages\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2301730 sha256=451014436aa121d86d289ba34052393a9d0c44b4b0f018bc1d12a069a7be5c8d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y23bp3v0/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
      "Successfully built kenlm\n"
     ]
    }
   ],
   "source": [
    "!pip3 install https://github.com/kpu/kenlm/archive/master.zip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = kenlm.LanguageModel(LM3)\n",
    "#model4 = kenlm.LanguageModel(LM4)\n",
    "#model5 = kenlm.LanguageModel(LM5)\n",
    "#натренована але нема місця щоб віддкрити"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model6 = kenlm.LanguageModel(LM6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences scores\n",
    "\n",
    "#### model.score(self, sentence, bos = True, eos = True)\n",
    "\n",
    "Return the __log10 probability of a string__.  By default, the string is treated as a sentence.  \n",
    "          \n",
    "          return log10 p(sentence </s> | <s>)\n",
    "\n",
    "If you do not want to condition on the beginning of sentence, pass __bos = False__ . Never include $<s>$ as part of the string. \n",
    "\n",
    "Similarly, the end of sentence token $</s>$ can be omitted with __eos = False__. Since language models explicitly predict $</s>$, it can be part of the string.\n",
    "\n",
    "\n",
    "I do not use bos or eos = False, so the method calculates scores of those strings to be sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Штучний інтелект врятує світ .\n",
      "3-gram model\n",
      "-13.396929740905762\n",
      "6-gram model\n",
      "-13.588444709777832\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Штучний інтелект врятує світ .'\n",
    "print(sentence1)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence1))\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(model4.score(sentence1))\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(model5.score(sentence1))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_#foreign_ врятує світ .\n",
      "3-gram model\n",
      "-10.425771713256836\n",
      "6-gram model\n",
      "-10.52933406829834\n"
     ]
    }
   ],
   "source": [
    "sentence2 = '_#foreign_ врятує світ .'\n",
    "print(sentence2)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence2))\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(model4.score(sentence2))\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(model5.score(sentence2))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наука врятує світ .\n",
      "3-gram model\n",
      "-12.745466232299805\n",
      "6-gram model\n",
      "-12.852709770202637\n"
     ]
    }
   ],
   "source": [
    "sentence3 = 'Наука врятує світ .'\n",
    "print(sentence3)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence3))\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(model4.score(sentence3))\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(model5.score(sentence3))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Краса врятує світ .\n",
      "3-gram model\n",
      "-8.470232963562012\n",
      "6-gram model\n",
      "-9.163093566894531\n"
     ]
    }
   ],
   "source": [
    "sentence4 = 'Краса врятує світ .'\n",
    "print(sentence4)\n",
    "print('{0}-gram model'.format(model3.order))\n",
    "print(model3.score(sentence4))\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(model4.score(sentence4))\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(model5.score(sentence4))\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(model6.score(sentence4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that total full score = direct score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model,s):\n",
    "    return sum(prob for prob, _, _ in model.full_scores(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (abs(score(model6, sentence1) - model6.score(sentence1)) < 1e-3)\n",
    "assert (abs(score(model6, sentence2) - model6.score(sentence2)) < 1e-3)\n",
    "assert (abs(score(model6, sentence3) - model6.score(sentence3)) < 1e-3)\n",
    "assert (abs(score(model6, sentence4) - model6.score(sentence4)) < 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show scores and n-gram matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.728082180023193 2: <s> Краса\n",
      "-2.4685885906219482 3: <s> Краса врятує\n",
      "-1.205726981163025 3: Краса врятує світ\n",
      "-0.7587704658508301 3: врятує світ .\n",
      "-0.0019260908011347055 4: врятує світ . </s>\n"
     ]
    }
   ],
   "source": [
    "words = ['<s>'] + sentence4.split() + ['</s>']\n",
    "for i, (prob, length, oov) in enumerate(model6.full_scores(sentence4)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gramm які закінчуютьмя на $</s>$ завжди друга цифра нуль. Виходить нема залежності між реченнями. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the perplexity of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, sentence, bos=True, eos=True):\n",
    "    \"\"\"\n",
    "    Compute perplexity of a sentence.\n",
    "    @param sentence One full sentence to score.  Do not include <s> or </s>.\n",
    "    \"\"\"\n",
    "    words = len(str(sentence).split()) + 1 # For </s>\n",
    "    return 10.0**(-model.score(sentence, bos=bos, eos=eos) / words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0171943001484\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(model6, sentence4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0171943001484\n"
     ]
    }
   ],
   "source": [
    "print(model6.perplexity(sentence4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if not w in model6:\n",
    "        print('\"{0}\" is an OOV'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perplexity of the model on the Ukrainian brown corpus (good and so-so)\n",
    "\n",
    "_!!! keep in mind this is the same dataset I made my estimation ARPA models on !!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=os.path.join(os.path.abspath(''), \n",
    "                      'brown-uk', 'corpus',\n",
    "                      'final_all_GS_tagged_words_symbols_sentences.txt')\n",
    "#read sentence by sentence\n",
    "temp = open(filename,'r').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only sentence-based model and it estimates score for each sentence separately. The score is equal to the  log10 probability of the sentence. Then I sum up scores for all the sentences in the corpus, divide them by the number of the words in the corpus and take 10 to the power of the resulting fraction to calculate the __perplexity__ of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_on_texts_by_sentences(model, temp):\n",
    "    all_score=0\n",
    "    all_words=0\n",
    "    for sentence in temp:\n",
    "        all_score+=model.score(sentence, bos = False, eos = False)\n",
    "        all_words+=len(str(sentence).split())\n",
    "    print(\"all_score: \"+str(all_score)+\"; \\nnumber of tokens in text: \"+str(all_words))\n",
    "    return 10.0**(-all_score / all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n",
      "all_score: -2319665.4281127453; \n",
      "number of tokens in text: 732024\n",
      "1475.1559840711334\n",
      "\n",
      "6-gram model\n",
      "all_score: -2288903.02160573; \n",
      "number of tokens in text: 732024\n",
      "1339.1036003071433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{0}-gram model'.format(model3.order))\n",
    "print(str(perplexity_on_texts_by_sentences(model3,temp))+\"\\n\")\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(str(perplexity_on_texts_by_sentences(model4,temp))+\"\\n\")\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(str(perplexity_on_texts_by_sentences(model5,temp))+\"\\n\")\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(str(perplexity_on_texts_by_sentences(model6,temp))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity including BOS and EOS tags. Copied their function but rewriten to match text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_on_texts_by_sentences_boseos(model, temp):\n",
    "    all_score=0\n",
    "    all_words=0\n",
    "    for sentence in temp:\n",
    "        if len(str(sentence))>0:\n",
    "            all_score+=model.score(sentence)\n",
    "            all_words+=len(str(sentence).split())+1\n",
    "        if len(str(sentence))==1:\n",
    "            all_words+=3\n",
    "        if len(str(sentence))==2:\n",
    "            all_words+=2\n",
    "        if len(str(sentence))==3:\n",
    "            all_words+=1\n",
    "        if len(str(sentence))==0:\n",
    "            all_words-=1\n",
    "    print(\"all_score: \"+str(all_score)+\"; \\nnumber of tokens in text: \"+str(all_words))\n",
    "    return 10.0**(-all_score / all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram model\n",
      "all_score: -2246459.9509153366; \n",
      "number of tokens in text: 771682\n",
      "814.9319723394601\n",
      "\n",
      "6-gram model\n",
      "all_score: -2212696.1555285454; \n",
      "number of tokens in text: 771682\n",
      "736.830929701527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{0}-gram model'.format(model3.order))\n",
    "print(str(perplexity_on_texts_by_sentences_boseos(model3,temp))+\"\\n\")\n",
    "#print('{0}-gram model'.format(model4.order))\n",
    "#print(str(perplexity_on_texts_by_sentences(model4,temp))+\"\\n\")\n",
    "#print('{0}-gram model'.format(model5.order))\n",
    "#print(str(perplexity_on_texts_by_sentences(model5,temp))+\"\\n\")\n",
    "print('{0}-gram model'.format(model6.order))\n",
    "print(str(perplexity_on_texts_by_sentences_boseos(model6,temp))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity computed by Kenlm query function\n",
    "\n",
    "#### 6-gram\n",
    "\n",
    "### without pruning\n",
    "\n",
    "#### size of arpa file = 41.1G\n",
    "#### training time 40minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This binary file contains probing hash tables.\n",
      "Name:query\tVmPeak:13718564 kB\tVmRSS:4776 kB\tRSSMax:13694864 kB\tuser:0.568461\tsys:0.680552\tCPU:1.24908\treal:1.24741\n"
     ]
    }
   ],
   "source": [
    "! kenlm/build/bin/query final/kenlm/uk_final_symbols_sentences_based_6.binary <brown-uk/corpus/final_all_GS_tagged_words_symbols_sentences.txt > uk_bruk_kenlm_6_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity including OOVs:\t736.830929701527\r\n",
      "Perplexity excluding OOVs:\t630.6853332748613\r\n",
      "OOVs:\t9174\r\n",
      "Tokens:\t771682\r\n"
     ]
    }
   ],
   "source": [
    "!tail -4 uk_bruk_kenlm_6_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity computed by Kenlm 3 query function\n",
    "\n",
    "3-gram\n",
    "\n",
    "### without pruning\n",
    "\n",
    "#### size of arpa file = 6.4G\n",
    "#### training time 3:55  minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading final/kenlm/uk_final_symbols_sentences_based_3.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:query\tVmPeak:3783640 kB\tVmRSS:4800 kB\tRSSMax:2711608 kB\tuser:66.3182\tsys:1.01585\tCPU:67.3341\treal:67.3396\n"
     ]
    }
   ],
   "source": [
    "! kenlm/build/bin/query final/kenlm/uk_final_symbols_sentences_based_3.arpa <brown-uk/corpus/final_all_GS_tagged_words_symbols_sentences.txt > uk_bruk_kenlm_3_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity including OOVs:\t814.9319723394601\r\n",
      "Perplexity excluding OOVs:\t697.8233563403668\r\n",
      "OOVs:\t9174\r\n",
      "Tokens:\t771682\r\n"
     ]
    }
   ],
   "source": [
    "!tail -4 uk_bruk_kenlm_3_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
