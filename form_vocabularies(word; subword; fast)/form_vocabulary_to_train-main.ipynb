{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "from utilities import *\n",
    "import re\n",
    "import random\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "txt_file_path = \"../../final_all_GS_tagged_words_symbols_sentences.txt\"\n",
    "txt_file_path0 = \"../../final_all_GS_tagged_words_symbols_sentences_0.txt\"\n",
    "txt_file_path1 = \"../../korr_final_symbols_sentences.txt\"\n",
    "txt_file_path2 = \"../../ukrlib_final_symbols_sentences.txt\" \n",
    "txt_file_path10 = \"../../korr_final_symbols_sentences_01.txt\"\n",
    "txt_file_path20 = \"../../ukrlib_final_symbols_sentences_01.txt\"\n",
    "txt_file_path100 = \"../../korr_final_symbols_sentences_00.txt\"\n",
    "txt_file_path200 = \"../../ukrlib_final_symbols_sentences_00.txt\"\n",
    "\n",
    "\n",
    "data_path = \"data/korr_ukrlib_data.pkl\"\n",
    "test_path = \"data/brown_test_data.pkl\"\n",
    "x_pad_data_path = \"data/korr_ukrlib_x_pad_data.pkl\"\n",
    "y_pad_data_path = \"data/korr_ukrlib_y_pad_data.pkl\"\n",
    "\n",
    "# Sentence tokens\n",
    "unknown_token = \"_#unknown_\"\n",
    "sentence_start_token = \"_#start_\"\n",
    "sentence_end_token = \"_#end_\"\n",
    "padding_token = \"_#padding_\"\n",
    "\n",
    "#Number of words to hold in vocabulary\n",
    "vocabulary_size = 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path2, \"rt\") as infile:\n",
    "    with open(txt_file_path20, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)\n",
    "                \n",
    "with open(txt_file_path1, \"rt\") as infile:\n",
    "    with open(txt_file_path10, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)\n",
    "with open(txt_file_path, \"rt\") as infile:\n",
    "    with open(txt_file_path0, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path10, \"rt\") as infile:\n",
    "    tokenized_sentences = [(\"%s %s %s\" % (sentence_start_token, sentence, sentence_end_token)).split() for sentence in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path20, \"rt\") as infile:\n",
    "    tokenized_sentences += [(\"%s %s %s\" % (sentence_start_token, sentence, sentence_end_token)).split() for sentence in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path0, \"rt\") as infile:\n",
    "    tokenized_sentences_test = [(\"%s %s %s\" % (sentence_start_token, sentence, sentence_end_token)).split() for sentence in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numwordsSE=0\n",
    "numwords=0\n",
    "for sent in tokenized_sentences:\n",
    "    numwords+=len(sent)-2\n",
    "    numwordsSE+=len(sent)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  14447614\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  270208921\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  241313693\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", numwordsSE) \n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \", numwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "for sentence in tokenized_sentences:\n",
    "    lengths.append(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.826231e+06, 6.102851e+06, 3.469397e+06, 1.356401e+06,\n",
       "        4.429190e+05, 1.495680e+05, 7.244400e+04, 1.579000e+04,\n",
       "        5.417000e+03, 2.564000e+03]),\n",
       " array([  0,  10,  20,  30,  40,  50,  60,  80, 100, 120, 140]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lengths, bins=[0,10,20,30,40,50,60,80,100,120,140])  # arguments are passed to np.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete sentences longer then 60 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_1 = [sentence for sentence in tokenized_sentences if (len(sentence) < 61 and len(sentence) > 3 ) ] \n",
    "tokenized_sentences=tokenized_sentences_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  14335495\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  262598163\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  233927173\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", sum([len(sen) for sen in tokenized_sentences]) )\n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \",  sum([len(sen)-2 for sen in tokenized_sentences])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  40382\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  812788\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  732024\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences_test))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", sum([len(sen) for sen in tokenized_sentences_test]) )\n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \",  sum([len(sen)-2 for sen in tokenized_sentences_test])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_#start_', 'Лідер', '\"', 'нацболів', '\"', 'Едуард', 'Лімонов', 'виступає', 'за', 'втручання', 'Росії', 'у', 'конфлікт', 'на', 'Донбасі', 'Лідер', 'забороненої', 'в', 'Росії', 'Націонал-більшовицької', 'партії', ',', 'що', 'нині', 'зібрав', 'своїх', 'прихильників', 'під', 'брендом', '\"', 'Інша', 'Росія', '\"', ',', 'письменник', 'Едуард', 'Лимонов', 'від', 'початку', 'конфлікту', 'в', 'Україні', 'повністю', 'підтримав', 'сепаратистські', 'настрої', 'на', 'Донбасі', 'і', 'регулярно', 'виступав', 'за', 'всебічне', 'втручання', 'Росії', 'в', 'цей', 'конфлікт', '.', '_#end_']\n"
     ]
    }
   ],
   "source": [
    "print(max(tokenized_sentences, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the word frequencies train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = nltk.FreqDist(itertools.chain(*tokenized_sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2189477 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab = word_frequency.most_common(len(word_frequency.items()))\n",
    "print(\"Found %d unique word tokens.\" % len(word_frequency.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the word frequencies test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_test = nltk.FreqDist(itertools.chain(*tokenized_sentences_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103337 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab_test = word_frequency_test.most_common(len(word_frequency_test.items()))\n",
    "print(\"Found %d unique word tokens.\" % len(word_frequency_test.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tokens in vocabulary that occured more then N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 times -  550437\n",
      "11 times -  368345\n",
      "14 times -  323726\n"
     ]
    }
   ],
   "source": [
    "print(\"5 times - \", len([i for i in vocab if i[1] > 5]))\n",
    "print(\"11 times - \", len([i for i in vocab if i[1] > 11]))\n",
    "print(\"14 times - \", len([i for i in vocab if i[1] > 14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the whole train vocabulary (sentences<60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7535"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the train vocabulary of 300 000 most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24931"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab[:300000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the train vocabulary of words that occured 15 and more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23630"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab[:323623]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete sentences longer then 60 tokens in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_test_1 = [sentence for sentence in tokenized_sentences_test if (len(sentence) < 61 and len(sentence) > 3 )] \n",
    "tokenized_sentences_test=tokenized_sentences_test_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  39900\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  779001\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  699201\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences_test))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", sum([len(sen) for sen in tokenized_sentences_test]) )\n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \",  sum([len(sen)-2 for sen in tokenized_sentences_test])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100420 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "word_frequency_test = nltk.FreqDist(itertools.chain(*tokenized_sentences_test)) \n",
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab_test = word_frequency_test.most_common(len(word_frequency_test.items()))\n",
    "print(\"Found %d unique word tokens.\" % len(word_frequency_test.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the whole train vocabulary (sentences<60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7161"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the train vocabulary of 300 000 most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23826"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab[:299999]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOV for the train vocabulary of words that occured 15 and more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab[:323623]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov=set([tuple_word[0] for tuple_word in vocab_test])-set([tup[0] for tup in vocab[:299999]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train vocabulary 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocab[:299999]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(',', 18621664)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word to index and index to words (Add the word not the frequency from our vocabulary data)\n",
    "index_to_word = [x[0] for x in vocabulary]\n",
    "index_to_word.insert(0, unknown_token)\n",
    "#index_to_word.insert(0, padding_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_#unknown_'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[0]\n",
    "#index_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of {word : index} pairs\n",
    "word_to_index = dict([(word, i) for i, word in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace all words not in our vocabulary with the #unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown=0\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sentence in enumerate(tokenized_sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in word_to_index:\n",
    "            tokenized_sentences[i][j] = unknown_token\n",
    "            unknown+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145722"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_test=0\n",
    "# RepB8bvjbx3mUQXnxB8bvjbx3mUQXnxlace all words not in our vocabulary with the unknown token\n",
    "for i, sentence in enumerate(tokenized_sentences_test):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in word_to_index:\n",
    "            tokenized_sentences_test[i][j] = unknown_token\n",
    "            unknown_test+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.append((unknown_token, unknown))\n",
    "#vocabulary.append((padding_token, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least frequent word in our vocabulary is 'наяві' and appeared 17 times.\n",
      "Example sentence after Pre-processing: '['_#start_', 'Як', 'вказують', 'експерти', ',', 'нині', 'основна', 'проблема', 'української', 'армії', '—', 'низький', 'рівень', 'грошового', 'забезпечення', 'та', 'відсутність', 'гарантій', 'соціальної', 'захищеності', '.', '_#end_']'\n"
     ]
    }
   ],
   "source": [
    "#print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (\n",
    "    vocabulary[-2][0], vocabulary[-2][1]))\n",
    "print(\"Example sentence after Pre-processing: '%s'\" % tokenized_sentences[101258])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form train data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for sentence in tokenized_sentences:\n",
    "    x = []\n",
    "    for word in sentence:\n",
    "        x.append(word_to_index[word])\n",
    "    x_train.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14335495"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(x_train)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "data = dict(\n",
    "    x_train=x_train,\n",
    "    #y_train=y_train,\n",
    "    word_to_index=word_to_index,\n",
    "    index_to_word=index_to_word,\n",
    "    vocabulary=vocabulary,\n",
    "    num_sentences=len(x_train),\n",
    "    max_input_len=len(max(x_train,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training data\n",
      "Saved file training data to data/korr_ukrlib_data.pkl.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving training data\")\n",
    "try:\n",
    "    save_training_data(data_path, data)\n",
    "except FileNotFoundError as err:\n",
    "    print(\"Error saving data \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form test data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for sentence in tokenized_sentences_test:\n",
    "    x = []\n",
    "    for word in sentence:\n",
    "        x.append(word_to_index[word])\n",
    "    x_test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(x_test)\n",
    "test_data = dict(\n",
    "    x_test=x_test,\n",
    "    oov=oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test data\n",
      "Saved file training data to data/brown_test_data.pkl.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving test data\")\n",
    "try:\n",
    "    save_training_data(test_path, test_data)\n",
    "except FileNotFoundError as err:\n",
    "    print(\"Error saving data \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef chunks(l, n):\\n    # For item i in a range that is a length of l,\\n    for i in range(0, len(l), n):\\n        # Create an index range for l of n items:\\n        yield l[i:i+n]\\n\\nx_train_new=[]\\ny_train_new=[]\\nfor somelist in x_train:\\n    if len(somelist)>100:\\n        somelist=list(chunks(somelist,100))\\n        for nested in somelist:\\n            x_train_new.append(nested)\\n    else:\\n        x_train_new.append(somelist)\\nfor somelist in y_train:\\n    if len(somelist)>100:\\n        somelist=list(chunks(somelist,100))\\n        for nested in somelist:\\n            y_train_new.append(nested)\\n    else:\\n        y_train_new.append(somelist) '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "x_train_new=[]\n",
    "y_train_new=[]\n",
    "for somelist in x_train:\n",
    "    if len(somelist)>100:\n",
    "        somelist=list(chunks(somelist,100))\n",
    "        for nested in somelist:\n",
    "            x_train_new.append(nested)\n",
    "    else:\n",
    "        x_train_new.append(somelist)\n",
    "for somelist in y_train:\n",
    "    if len(somelist)>100:\n",
    "        somelist=list(chunks(somelist,100))\n",
    "        for nested in somelist:\n",
    "            y_train_new.append(nested)\n",
    "    else:\n",
    "        y_train_new.append(somelist) \"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
