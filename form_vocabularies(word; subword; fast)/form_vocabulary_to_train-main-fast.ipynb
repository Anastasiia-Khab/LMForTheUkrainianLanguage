{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form vocabulary from FastText pretrained vectors and our train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from utilities import *\n",
    "import re\n",
    "import io\n",
    "import random\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "txt_file_path = \"../../final_all_GS_tagged_words_symbols_sentences.txt\"\n",
    "txt_file_path0 = \"../../final_all_GS_tagged_words_symbols_sentences_0.txt\"\n",
    "txt_file_path00 = \"../../final_all_GS_tagged_words_symbols_sentences_00.txt\"\n",
    "txt_file_path1 = \"../../korr_final_symbols_sentences.txt\"\n",
    "txt_file_path2 = \"../../ukrlib_final_symbols_sentences.txt\" \n",
    "txt_file_path10 = \"../../korr_final_symbols_sentences_01.txt\"\n",
    "txt_file_path20 = \"../../ukrlib_final_symbols_sentences_01.txt\"\n",
    "txt_file_path100 = \"../../korr_final_symbols_sentences_00.txt\"\n",
    "txt_file_path200 = \"../../ukrlib_final_symbols_sentences_00.txt\"\n",
    "\n",
    "txt_file_path300 = \"../../korr_ukrlib_final_symbols_sentences_00.txt\"\n",
    "\n",
    "\n",
    "data_path = \"data/korr_ukrlib_data_fast.pkl\"\n",
    "test_path = \"data/brown_test_data_fast.pkl\"\n",
    "\n",
    "# Sentence tokens\n",
    "unknown_token = \"_#unknown_\"\n",
    "sentence_start_token = \"_#start_\"\n",
    "sentence_end_token = \"_#end_\"\n",
    "padding_token = \"_#padding_\"\n",
    "\n",
    "#Number of words to hold in vocabulary\n",
    "vocabulary_size = 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Loaded file training data from data/korr_ukrlib_data_fast.pkl.\n",
      "Loaded file training data from data/brown_test_data_fast.pkl.\n",
      "DONE loading data ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data ...\")\n",
    "data = load_training_data(data_path)\n",
    "data_test = load_training_data(test_path)\n",
    "print(\"DONE loading data ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['index_to_fast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299999"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sen) for sen in data['x_train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(sen) for sen in data['x_train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path2, \"rt\") as infile:\n",
    "    with open(txt_file_path20, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)\n",
    "                \n",
    "with open(txt_file_path1, \"rt\") as infile:\n",
    "    with open(txt_file_path10, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)\n",
    "with open(txt_file_path, \"rt\") as infile:\n",
    "    with open(txt_file_path0, \"w+\") as outfile:\n",
    "        for sent in infile:\n",
    "            if len(sent.split())>58:\n",
    "                sep=';'\n",
    "                separ='////'\n",
    "                for k in sent.replace(sep, sep+separ).split(separ):\n",
    "                    print(re.sub( r'^   |\\n|^\\t','',k), file=outfile)\n",
    "            else:\n",
    "                outfile.write(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path300, \"rt\") as infile:\n",
    "    tokenized_sentences = [(\"%s %s %s\" % (sentence_start_token, sentence, sentence_end_token)).split() for sentence in infile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path00, \"rt\") as infile:\n",
    "    tokenized_sentences_test = [(\"%s %s %s\" % (sentence_start_token, sentence, sentence_end_token)).split() for sentence in infile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train corpus sentences<60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  14335495\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  262598163\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  233927173\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", sum([len(sen) for sen in tokenized_sentences]) )\n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \",  sum([len(sen)-2 for sen in tokenized_sentences])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test corpus sentences <60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences  39900\n",
      "Number of all tokens (with _#start_ _#end_ in each sentence)  779001\n",
      "Number of all tokens (without _#start_ _#end_ in each sentence)  699201\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of sentences \", len(tokenized_sentences_test))\n",
    "print(\"Number of all tokens (with _#start_ _#end_ in each sentence) \", sum([len(sen) for sen in tokenized_sentences_test]) )\n",
    "print(\"Number of all tokens (without _#start_ _#end_ in each sentence) \",  sum([len(sen)-2 for sen in tokenized_sentences_test])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of the longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_#start_', '\"', 'Одеська', '\"', 'вирячила', 'здуру', 'на', 'молодика', ',', 'що', 'зазирав', 'одним', 'рогом', 'у', 'вікно', ',', 'свої', 'сіро-зелені', 'баньки', ',', 'раптом', ',', 'немов', 'читаючи', 'на', 'небі', 'ноти', ',', 'заверещала', 'тоненьким', 'голоском', ',', 'наче', 'сука', 'на', 'місяць', ':', 'Спи', 'же', ',', 'спи', ',', 'моя', 'родная', ',', 'Бог', 'твой', 'сон', 'хранит', ',', 'Твоя', 'мама', 'шансонетка', ',', 'По', 'ночам', 'не', 'спит', '...', '_#end_']\n"
     ]
    }
   ],
   "source": [
    "print(max(tokenized_sentences, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the word frequencies train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = nltk.FreqDist(itertools.chain(*tokenized_sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2189477 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab = word_frequency.most_common(len(word_frequency.items()))\n",
    "print(\"Found %d unique word tokens.\" % len(word_frequency.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the word frequencies test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_test = nltk.FreqDist(itertools.chain(*tokenized_sentences_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_frequency_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100420 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index to word and word to index vectors\n",
    "vocab_test = word_frequency_test.most_common(len(word_frequency_test.items()))\n",
    "print(\"Found %d unique word tokens.\" % len(word_frequency_test.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train_fast=[]\n",
    "vocabkeys = [pair[0].replace(\"'\",'') for pair in vocab]\n",
    "vocabkeys2 = [pair[0] for pair in vocab]\n",
    "\n",
    "vocabkeys_set = set(vocabkeys)\n",
    "vocabkeys2_set = set(vocabkeys2)\n",
    "vocabkeys_map = dict()\n",
    "for step, vocab_key in enumerate(vocabkeys):\n",
    "    vocabkeys_map[vocab_key] = step\n",
    "vocab_dict=dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "present=[]\n",
    "absent=[]\n",
    "fin = io.open('../../cc.uk.300.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in vocabkeys2_set:\n",
    "            vocab_train_fast.append((tokens[0],vocab_dict[tokens[0]],list(map(float, tokens[1:]))))\n",
    "            present.append(tokens[0])\n",
    "        elif tokens[0] in vocabkeys_set:\n",
    "            vocab_train_fast.append((vocabkeys2[vocabkeys_map[tokens[0]]],vocab_dict[vocabkeys2[vocabkeys_map[tokens[0]]]],list(map(float, tokens[1:]))))\n",
    "            present.append(tokens[0])\n",
    "        else:\n",
    "            absent.append(tokens[0])\n",
    "        #vocab.append(tokens[0])\n",
    "    #return vocab, np.stack(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "absent_in_fast=vocabkeys2_set-set(present)-set([sentence_end_token, sentence_start_token])\n",
    "unk_freak=sum([vocab_dict[v] for v in absent_in_fast])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10335097"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_freak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763578"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_train_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=-1\n",
    "fin = io.open('../../cc.uk.300.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    i+=1\n",
    "    #print('yes')\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    #data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    if i==1:\n",
    "        unk=np.array(list(map(float, tokens[1:])))\n",
    "    elif i>1:\n",
    "        unk+=np.array(list(map(float, tokens[1:])))\n",
    "        unk/2\n",
    "#vec2=vect/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "r=0\n",
    "fin = io.open('../../cc.uk.300.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    r+=1\n",
    "    #print('yes')\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    #data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    if i==0 and tokens[0].isdigit() and r>1:\n",
    "        i+=1\n",
    "        numbers=np.array(list(map(float, tokens[1:])))\n",
    "    elif i>0 and tokens[0].isdigit():\n",
    "        numbers+=np.array(list(map(float, tokens[1:])))\n",
    "        numbers/2\n",
    "#vec2=vect/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train_fast.append(('_#number_',vocab_dict['_#number_'],list(numbers)))\n",
    "vocab_train_fast.append(('_#unknown_',unk_freak,list(unk)))\n",
    "vocab_train_fast.append((sentence_end_token,vocab_dict[sentence_end_token],[random.uniform(-1,1) for i in range(300)]))\n",
    "vocab_train_fast.append((sentence_start_token,vocab_dict[sentence_start_token],[random.uniform(-1,1) for i in range(300)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train_fast.sort(key=lambda tup: tup[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763582"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_train_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2182152"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['_#number_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train vocabulary 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocab_train_fast[:300000]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('_#unknown_',\n",
       " 10335097,\n",
       " [42152.47709999825,\n",
       "  -2113.3825999999785,\n",
       "  1532.3419000000683,\n",
       "  39876.290299997694,\n",
       "  -11239.409200000297,\n",
       "  -3118.373800000167,\n",
       "  -12070.744099999265,\n",
       "  -31521.033399998738,\n",
       "  -33498.35169999813,\n",
       "  -60447.93899999902,\n",
       "  47054.68559999562,\n",
       "  1777.295400000074,\n",
       "  -38.067699999966635,\n",
       "  -45705.3566999965,\n",
       "  18276.20599999667,\n",
       "  3552.121100000272,\n",
       "  -741.4504999999903,\n",
       "  -7665.517900000761,\n",
       "  4476.051000000697,\n",
       "  44728.22319999629,\n",
       "  17941.48070000008,\n",
       "  -55487.12080000034,\n",
       "  13841.112899999493,\n",
       "  1755.1929000001148,\n",
       "  -7676.18930000046,\n",
       "  10327.149999999703,\n",
       "  -7216.319699997982,\n",
       "  26088.255399996575,\n",
       "  1983.6393999999063,\n",
       "  -2588.206700000004,\n",
       "  -7571.823200001247,\n",
       "  1787.8362999999604,\n",
       "  4.570400000022857,\n",
       "  -1279.525899999928,\n",
       "  10869.424400000717,\n",
       "  5077.936100000569,\n",
       "  -780.6250000000095,\n",
       "  26614.84829999766,\n",
       "  11797.106799999781,\n",
       "  22085.686299997047,\n",
       "  -18963.91879999885,\n",
       "  -3651.871100000314,\n",
       "  223.08250000001686,\n",
       "  3049.9829999999347,\n",
       "  2818.036299999823,\n",
       "  4956.547900000461,\n",
       "  -11997.232699999482,\n",
       "  -1007.4963000004856,\n",
       "  250.03369999998858,\n",
       "  -4329.642700000005,\n",
       "  7474.722600001272,\n",
       "  -36577.58709999789,\n",
       "  5310.879700000632,\n",
       "  1989.8811000001238,\n",
       "  -4861.425100000231,\n",
       "  45211.874499997655,\n",
       "  5513.775800000753,\n",
       "  4203.0469000002895,\n",
       "  -17394.110199998766,\n",
       "  -32968.69979999265,\n",
       "  8094.658500001396,\n",
       "  -9088.388400000968,\n",
       "  5521.272200001001,\n",
       "  -3330.5245000001523,\n",
       "  6792.477700001257,\n",
       "  -23653.10379999905,\n",
       "  1959.5502999999708,\n",
       "  -26553.798199998095,\n",
       "  -975.4255999999945,\n",
       "  -3806.321900000134,\n",
       "  21880.104199998685,\n",
       "  190.02529999999174,\n",
       "  8320.893800001928,\n",
       "  333.895800000074,\n",
       "  3064.1097999998874,\n",
       "  -1494.6951000000265,\n",
       "  117.17149999998405,\n",
       "  -2647.2524999999155,\n",
       "  -2706.672099999926,\n",
       "  6967.303200000733,\n",
       "  -2439.3789000001443,\n",
       "  3219.5534999999613,\n",
       "  -1933.7279000000547,\n",
       "  8718.942400001177,\n",
       "  6830.281400001217,\n",
       "  26950.004499998075,\n",
       "  10406.43329999993,\n",
       "  -23469.007399999035,\n",
       "  3489.6175000002263,\n",
       "  7222.359999999536,\n",
       "  -1218.5897999999875,\n",
       "  11274.363999998977,\n",
       "  -47721.81279999885,\n",
       "  12332.218299999839,\n",
       "  -1346.08299999999,\n",
       "  22945.72329999856,\n",
       "  -3170.5279999999934,\n",
       "  933.903399999989,\n",
       "  -8336.418800001964,\n",
       "  10254.524900000222,\n",
       "  -30919.628500000716,\n",
       "  -4764.90320000091,\n",
       "  254.9091999999651,\n",
       "  -5000.431500000615,\n",
       "  3937.488100000546,\n",
       "  1200.6793000000043,\n",
       "  46170.13279999838,\n",
       "  3184.4789999998925,\n",
       "  -4234.173599999928,\n",
       "  -1842.3063000000893,\n",
       "  -5437.750400000494,\n",
       "  6739.818800001145,\n",
       "  -8071.386800001823,\n",
       "  3899.569799999918,\n",
       "  3296.409700000227,\n",
       "  12257.242099999714,\n",
       "  -22425.47949999919,\n",
       "  -7743.234700002066,\n",
       "  9134.23020000049,\n",
       "  3632.0252000002997,\n",
       "  7179.876900001762,\n",
       "  -4439.549200000088,\n",
       "  13814.687399999966,\n",
       "  -9025.87330000113,\n",
       "  -488.0116999999763,\n",
       "  66261.92679999713,\n",
       "  6680.332800001117,\n",
       "  -2462.4600000000783,\n",
       "  -12023.367599998825,\n",
       "  2930.1070000002687,\n",
       "  1929.169900000002,\n",
       "  17892.870300000024,\n",
       "  -786.645800000034,\n",
       "  -2443.279999999912,\n",
       "  55027.480199999445,\n",
       "  -4157.243699999929,\n",
       "  22942.91289999811,\n",
       "  5124.705500000888,\n",
       "  -16682.599599999903,\n",
       "  -8851.53150000004,\n",
       "  1744.34470000009,\n",
       "  3419.281200000096,\n",
       "  -6680.292000001283,\n",
       "  6387.405100001297,\n",
       "  -39175.364699996884,\n",
       "  4480.628100000321,\n",
       "  -3020.8827999998566,\n",
       "  -1015.3353,\n",
       "  -10022.721800000741,\n",
       "  -6025.3705000013315,\n",
       "  7649.460000001637,\n",
       "  4433.088899999808,\n",
       "  -1538.5891999999617,\n",
       "  -1435.149600000223,\n",
       "  9634.35579999985,\n",
       "  -11132.421500000113,\n",
       "  -4283.6600000001245,\n",
       "  -30024.09709999906,\n",
       "  -8538.819500002337,\n",
       "  10508.263600000015,\n",
       "  840.4930999999865,\n",
       "  624.9420000000249,\n",
       "  -7768.996700002658,\n",
       "  2000.9409999999266,\n",
       "  195739.58320001324,\n",
       "  4455.780199999665,\n",
       "  23971.32199999873,\n",
       "  55618.62030000213,\n",
       "  11653.93209999984,\n",
       "  -3627.968200000113,\n",
       "  3049.8512000000355,\n",
       "  -5879.158600001012,\n",
       "  189.11400000000438,\n",
       "  1457.1916000000604,\n",
       "  -1423.0015000000801,\n",
       "  171.19310000004256,\n",
       "  -2408.782600000063,\n",
       "  3582.6963999999,\n",
       "  -13653.151999998157,\n",
       "  -10638.79280000031,\n",
       "  -2558.1734999999135,\n",
       "  41086.772200000916,\n",
       "  2797.6527000001106,\n",
       "  -6533.699300001228,\n",
       "  31907.759999997903,\n",
       "  -17203.330000001242,\n",
       "  -10224.447400000603,\n",
       "  -12325.49119999973,\n",
       "  22004.222499998723,\n",
       "  -10065.71630000144,\n",
       "  -2553.281599999858,\n",
       "  -2839.081600000077,\n",
       "  14680.361299998096,\n",
       "  -2862.6223000000227,\n",
       "  3517.394999999936,\n",
       "  10731.471300000743,\n",
       "  50253.95799999925,\n",
       "  2154.7800999999904,\n",
       "  38771.71079999721,\n",
       "  43151.27469999915,\n",
       "  -16121.885699999004,\n",
       "  6439.6567000009445,\n",
       "  -40249.01010000187,\n",
       "  -5736.944100000226,\n",
       "  3155.0471999997408,\n",
       "  2845.3356999998227,\n",
       "  15419.278699997327,\n",
       "  972.1415999999771,\n",
       "  -2971.5302999999267,\n",
       "  -9307.606600000829,\n",
       "  18121.84189999925,\n",
       "  -10902.131500000673,\n",
       "  38408.81889999184,\n",
       "  5136.706500000395,\n",
       "  7997.185799999351,\n",
       "  -10696.555700000617,\n",
       "  -6320.824600000283,\n",
       "  1888.2285999999422,\n",
       "  1180.8185000000808,\n",
       "  -3696.684700000404,\n",
       "  -1239.5171999999811,\n",
       "  -3421.9401000001176,\n",
       "  -3189.3483000000456,\n",
       "  -3607.404500000146,\n",
       "  32400.30169999679,\n",
       "  -14454.25999999937,\n",
       "  -5295.836500000611,\n",
       "  3783.4149000001016,\n",
       "  -2136.523000000072,\n",
       "  -2157.8430000000817,\n",
       "  3065.142499999852,\n",
       "  -6440.174400000105,\n",
       "  -1897.504600000042,\n",
       "  -23004.634799995714,\n",
       "  6528.275900000249,\n",
       "  2805.7341000001234,\n",
       "  4200.790300000234,\n",
       "  1605.661799999942,\n",
       "  3933.207399999671,\n",
       "  7825.088700001538,\n",
       "  21428.27270000015,\n",
       "  4677.043400000289,\n",
       "  17428.357499998292,\n",
       "  -32915.84499999913,\n",
       "  -2246.249099999893,\n",
       "  -1396.8338000000372,\n",
       "  -2546.589900000009,\n",
       "  -11541.77309999994,\n",
       "  -594.8608000000656,\n",
       "  1709.343100000038,\n",
       "  5015.582400000251,\n",
       "  3371.716000000366,\n",
       "  -3208.599700000097,\n",
       "  -9561.397900000025,\n",
       "  -6629.675400000744,\n",
       "  8092.556400002075,\n",
       "  3304.4781000000107,\n",
       "  3960.079899999948,\n",
       "  19874.720899998334,\n",
       "  -3565.144399999894,\n",
       "  -2947.8148999997893,\n",
       "  4444.672600000152,\n",
       "  -28856.351400002535,\n",
       "  -15687.70070000222,\n",
       "  -1607.5101999997773,\n",
       "  316.0606999999832,\n",
       "  3070.964800000055,\n",
       "  480.52740000005844,\n",
       "  25776.537399999303,\n",
       "  38991.5546000013,\n",
       "  10868.566299999642,\n",
       "  5712.846100000985,\n",
       "  -27677.801199999252,\n",
       "  4103.497000000048,\n",
       "  45031.11779999899,\n",
       "  11164.38729999989,\n",
       "  -2020.3085000000708,\n",
       "  -3052.291299999905,\n",
       "  7791.449600001927,\n",
       "  -2791.1005999994736,\n",
       "  -57148.21880000587,\n",
       "  -2711.025999999782,\n",
       "  2968.081000000017,\n",
       "  -2339.747100000006,\n",
       "  -25408.58599999964,\n",
       "  -2416.5704999999584,\n",
       "  5141.092700000439,\n",
       "  -8482.077000001771,\n",
       "  5728.367200000886,\n",
       "  2405.6972999998643,\n",
       "  10276.204800000713,\n",
       "  4288.250099999865,\n",
       "  -20947.113400000544,\n",
       "  -4245.335300000313,\n",
       "  -2244.5921999999805,\n",
       "  3371.598799999946,\n",
       "  8644.16850000068,\n",
       "  -786.666599999968,\n",
       "  5737.2257000007885,\n",
       "  -1285.9699000000278])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word to index and index to words (Add the word not the frequency from our vocabulary data)\n",
    "index_to_word = [x[0] for x in vocabulary]\n",
    "#index_to_word.insert(0, unknown_token)\n",
    "#index_to_word.insert(0, padding_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[0]\n",
    "#index_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of {word : index} pairs\n",
    "word_to_index = dict([(word, i) for i, word in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_fast=np.stack([x[2] for x in vocabulary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace all words not in our vocabulary with the #unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown=0\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sentence in enumerate(tokenized_sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in word_to_index:\n",
    "            tokenized_sentences[i][j] = unknown_token\n",
    "            unknown+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9820363"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_test=0\n",
    "# RepB8bvjbx3mUQXnxB8bvjbx3mUQXnxlace all words not in our vocabulary with the unknown token\n",
    "for i, sentence in enumerate(tokenized_sentences_test):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in word_to_index:\n",
    "            tokenized_sentences_test[i][j] = unknown_token\n",
    "            unknown_test+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35187"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[4]=(vocabulary[4][0], unknown, vocabulary[4][2])\n",
    "#vocabulary.append((padding_token, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least frequent word in our vocabulary is 'Телевізійну' and appeared 14 times.\n",
      "Example sentence after Pre-processing: '['_#start_', '—', 'Ти', 'завтра', 'їдеш', 'на', 'озеро', '?', '—', 'запитала', 'вже', 'зовсім', 'приязно', '.', '_#end_']'\n"
     ]
    }
   ],
   "source": [
    "#print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (\n",
    "    vocabulary[-2][0], vocabulary[-2][1]))\n",
    "print(\"Example sentence after Pre-processing: '%s'\" % tokenized_sentences[101265])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form train data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for sentence in tokenized_sentences:\n",
    "    x = []\n",
    "    for word in sentence:\n",
    "        x.append(word_to_index[word])\n",
    "    x_train.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14335495"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(x_train)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "data = dict(\n",
    "    x_train=x_train,\n",
    "    #y_train=y_train,\n",
    "    word_to_index=word_to_index,\n",
    "    index_to_word=index_to_word,\n",
    "    index_to_fast=index_to_fast,\n",
    "    num_sentences=len(x_train),\n",
    "    max_input_len=len(max(x_train,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training data\n",
      "Saved file training data to data/korr_ukrlib_data_fast.pkl.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving training data\")\n",
    "try:\n",
    "    save_training_data(data_path, data)\n",
    "except FileNotFoundError as err:\n",
    "    print(\"Error saving data \" + str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form test data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for sentence in tokenized_sentences_test:\n",
    "    x = []\n",
    "    for word in sentence:\n",
    "        x.append(word_to_index[word])\n",
    "    x_test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(x_test)\n",
    "test_data = dict(\n",
    "    x_test=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test data\n",
      "Saved file training data to data/brown_test_data_fast.pkl.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving test data\")\n",
    "try:\n",
    "    save_training_data(test_path, test_data)\n",
    "except FileNotFoundError as err:\n",
    "    print(\"Error saving data \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef chunks(l, n):\\n    # For item i in a range that is a length of l,\\n    for i in range(0, len(l), n):\\n        # Create an index range for l of n items:\\n        yield l[i:i+n]\\n\\nx_train_new=[]\\ny_train_new=[]\\nfor somelist in x_train:\\n    if len(somelist)>100:\\n        somelist=list(chunks(somelist,100))\\n        for nested in somelist:\\n            x_train_new.append(nested)\\n    else:\\n        x_train_new.append(somelist)\\nfor somelist in y_train:\\n    if len(somelist)>100:\\n        somelist=list(chunks(somelist,100))\\n        for nested in somelist:\\n            y_train_new.append(nested)\\n    else:\\n        y_train_new.append(somelist) '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "\n",
    "x_train_new=[]\n",
    "y_train_new=[]\n",
    "for somelist in x_train:\n",
    "    if len(somelist)>100:\n",
    "        somelist=list(chunks(somelist,100))\n",
    "        for nested in somelist:\n",
    "            x_train_new.append(nested)\n",
    "    else:\n",
    "        x_train_new.append(somelist)\n",
    "for somelist in y_train:\n",
    "    if len(somelist)>100:\n",
    "        somelist=list(chunks(somelist,100))\n",
    "        for nested in somelist:\n",
    "            y_train_new.append(nested)\n",
    "    else:\n",
    "        y_train_new.append(somelist) \"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
